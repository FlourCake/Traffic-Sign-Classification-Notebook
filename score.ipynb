{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import & Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basePath = os.getcwd()\n",
    "trainPath = os.path.join(os.getcwd(), \"Images\")\n",
    "testPath = os.path.join(os.getcwd(), \"Online-Test-sort\")\n",
    "xlsFile = os.path.join(os.getcwd(), \"modelList.xlsx\")\n",
    "print(trainPath)\n",
    "print(testPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 128\n",
    "width = 128\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_modellist(path):\n",
    "    modelList = pd.DataFrame(columns=['model', 'accuracy', 'loss', 'val_accuracy', 'val_loss', 'test_accuracy', 'avg_precision', 'avg_recall', 'avg_f1_score'])\n",
    "    \n",
    "    for x in os.listdir(path):\n",
    "        if x.endswith('.h5'):\n",
    "            modelList = pd.concat([modelList, pd.DataFrame([[x.split('.')[0], 0, 0, 0, 0, 0, 0, 0, 0]], columns=['model', 'accuracy', 'loss', 'val_accuracy', 'val_loss', 'test_accuracy', 'avg_precision', 'avg_recall', 'avg_f1_score'])], ignore_index=True)\n",
    "        elif x.endswith('.pickle'):\n",
    "            with open(os.path.join(path, x), 'rb') as file_pi:\n",
    "                data = pickle.load(file_pi)\n",
    "                modelList.loc[modelList['model'] == x.split('.')[0], 'accuracy'] = data['accuracy'][-1]\n",
    "                modelList.loc[modelList['model'] == x.split('.')[0], 'loss'] = data['loss'][-1]\n",
    "                \n",
    "                if 'val_accuracy' in data:\n",
    "                    modelList.loc[modelList['model'] == x.split('.')[0], 'val_accuracy'] = data['val_accuracy'][-1]\n",
    "                    modelList.loc[modelList['model'] == x.split('.')[0], 'val_loss'] = data['val_loss'][-1]\n",
    "\n",
    "                # EvaluateModel(x.split('.')[0], data)\n",
    "    \n",
    "    return modelList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(path, batch_size):\n",
    "    test_datagen = ImageDataGenerator()\n",
    "    batch_size = batch_size\n",
    "\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        path,\n",
    "        target_size=(height, width),\n",
    "        color_mode=\"rgb\",\n",
    "        shuffle = False,\n",
    "        class_mode='categorical',\n",
    "        batch_size=batch_size)\n",
    "\n",
    "    labels = test_generator.classes\n",
    "    filenames = test_generator.filenames\n",
    "    nb_samples = np.ceil(len(filenames)/batch_size)\n",
    "\n",
    "    return test_generator, nb_samples, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confMatrix(path, modelName, labels, pred):\n",
    "    cf = confusion_matrix(labels, pred)\n",
    "    plt.clf()\n",
    "    sns.heatmap(cf, annot=True, cmap=\"Blues\")\n",
    "    plt.savefig(os.path.join(path, (modelName + '-confusion-matrix.png')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classificationReport(path, modelName, labels, pred):\n",
    "    cr = classification_report(labels, pred, output_dict=True)\n",
    "\n",
    "    df = pd.DataFrame(cr).transpose()\n",
    "    df.to_excel(os.path.join(path, (modelName + '-classification-report.xlsx')))\n",
    "\n",
    "    precicion = df.loc['macro avg', 'precision']\n",
    "    recall = df.loc['macro avg', 'recall']\n",
    "    f1_score = df.loc['macro avg', 'f1-score']\n",
    "\n",
    "    return precicion, recall, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(path, pathTest, modelList, max_accuracy, max_accuracy_model, max_accuracy_pred):\n",
    "    plt.figure(figsize = (20,20))\n",
    "    sns.set(font_scale=1.4)\n",
    "\n",
    "    for x in modelList['model']:\n",
    "        model = load_model(os.path.join(path, (x + '.h5')))\n",
    "        config = model.get_config()\n",
    "        config = config[\"layers\"][0][\"config\"][\"batch_input_shape\"]\n",
    "        \n",
    "        if ((config[1] != height) or (config[2] != width)):\n",
    "            print(\"Different input size!\")\n",
    "            print(\"Expected: {}x{}\".format(config[1],config[2]))\n",
    "            print(\"Given: {}x{}\".format(height, width))\n",
    "            print(\"Model name:{}\".format(x))\n",
    "            continue\n",
    "\n",
    "        batch_size = x.split('-')[1]\n",
    "        batch_size = int(batch_size)\n",
    "\n",
    "        test_generator, nb_samples, labels = data_generator(pathTest, batch_size)\n",
    "\n",
    "        if nb_samples is not None:\n",
    "            predict_x = model.predict(test_generator, steps=nb_samples)\n",
    "        else:\n",
    "            predict_x = model.predict(test_generator)\n",
    "\n",
    "        pred = np.argmax(predict_x, axis=1)\n",
    "\n",
    "        print(x, config[2])\n",
    "\n",
    "        accuracy = accuracy_score(labels, pred)\n",
    "        print('Test Data accuracy: ', accuracy * 100)\n",
    "\n",
    "        modelList.loc[modelList['model'] == x, 'test_accuracy'] = accuracy\n",
    "\n",
    "        confMatrix(path, x, labels, pred)\n",
    "        precicion, recall, f1_score = classificationReport(path, x, labels, pred)\n",
    "\n",
    "        modelList.loc[modelList['model'] == x, 'avg_precision'] = precicion\n",
    "        modelList.loc[modelList['model'] == x, 'avg_recall'] = recall\n",
    "        modelList.loc[modelList['model'] == x, 'avg_f1_score'] = f1_score\n",
    "\n",
    "        if accuracy > max_accuracy:\n",
    "            max_accuracy = accuracy\n",
    "            max_accuracy_model = x\n",
    "            max_accuracy_pred = pred\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "    print('Max accuracy: ', max_accuracy * 100)\n",
    "    print('Max accuracy model: ', max_accuracy_model)\n",
    "\n",
    "    return max_accuracy, max_accuracy_model, max_accuracy_pred, modelList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelList = pd.DataFrame(columns=['model', 'accuracy', 'loss', 'val_accuracy', 'val_loss', 'test_accuracy', 'avg_precision', 'avg_recall', 'avg_f1_score'])\n",
    "enhance_type = ['none', 'gaussian', 'sharpen', 'clahe', 'combo', 'robust']\n",
    "# enhance_type = ['robust']\n",
    "max_accuracy = 0\n",
    "max_accuracy_model = ''\n",
    "max_accuracy_pred = []\n",
    "\n",
    "for x in enhance_type:\n",
    "    pathModel = os.path.join(basePath, (x + '-model'))\n",
    "    if x == 'none':\n",
    "        pathTest = testPath\n",
    "    else:\n",
    "        pathTest = os.path.join(basePath, (x + '-test'))\n",
    "\n",
    "    print(pathModel, pathTest)\n",
    "\n",
    "    tempModelList = create_modellist(pathModel)\n",
    "    max_accuracy, max_accuracy_model, max_accuracy_pred, tempModelList = predict(pathModel, pathTest, tempModelList, max_accuracy, max_accuracy_model, max_accuracy_pred)\n",
    "\n",
    "    modelList = pd.concat([modelList, tempModelList])\n",
    "\n",
    "modelList.to_excel(xlsFile, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = max_accuracy_model.split('-')[1]\n",
    "batch_size = int(batch_size)\n",
    "enhance = max_accuracy_model.split('-')[4]\n",
    "\n",
    "if enhance == 'none':\n",
    "    pathModel = os.path.join(basePath, (x + '-model'))\n",
    "    pathTest = testPath\n",
    "else:\n",
    "    pathModel = os.path.join(basePath, (enhance + '-model'))\n",
    "    pathTest = os.path.join(basePath, (enhance + '-test'))\n",
    "\n",
    "test_generator, nb_samples, labels = data_generator(pathTest, batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Confusion Matrix & Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = confusion_matrix(labels, max_accuracy_pred)\n",
    "plt.figure(figsize = (25,25))\n",
    "sns.set(font_scale=1.4)\n",
    "sns.heatmap(cf, annot=True, cmap=\"Blues\", fmt='g')\n",
    "# plt.savefig(os.path.join(pathModel, (max_accuracy_model + '-confusion-matrix.png')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = classification_report(labels, max_accuracy_pred)\n",
    "print(cr)\n",
    "\n",
    "# cr.to_excel(\"classification-report.xlsx\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (25, 25))\n",
    "\n",
    "start_index = 0\n",
    "for i in range(25):\n",
    "    j = random.randint(1, len(labels))\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    prediction = max_accuracy_pred[start_index + j]\n",
    "    actual = labels[start_index + j]\n",
    "    col = 'g'\n",
    "    if prediction != actual:\n",
    "        col = 'r'\n",
    "    plt.xlabel('Actual={} || Pred={}'.format(actual, prediction), color = col)\n",
    "    plt.imshow(cv2.imread(os.path.join(test_generator.directory,test_generator.filenames[start_index + j])))\n",
    "    plt.savefig(os.path.join(pathModel, (max_accuracy_model + '-best-accuracy.png')))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TugasAkhir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
